{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYf3x-4gRUxf",
        "outputId": "701d31aa-04b4-4828-a576-57eb5800ff56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "\n",
        "# SETTINGS\n",
        "NUM_CLIENTS = 10\n",
        "CLIENT_DATA_PATH = \"/content/drive/Shareddrives/ML4Net/Seminar5/dataset_Seminar5/client_datasets/\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS_LOCAL = 3\n",
        "ROUNDS = 5\n",
        "CLIENTS_PER_ROUND = 5\n",
        "LR = 0.01\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Step 1: Collect all labels globally to build global label mapping\n",
        "all_labels = []\n",
        "\n",
        "for client_id in range(1, NUM_CLIENTS + 1):\n",
        "    labels_path = os.path.join(CLIENT_DATA_PATH, f\"client_{client_id}_labels.csv\")\n",
        "    if os.path.exists(labels_path):\n",
        "        y = pd.read_csv(labels_path, header=None).values.flatten()\n",
        "        all_labels.extend(y)\n",
        "    else:\n",
        "        print(f\"âŒ Missing labels file for client {client_id} â€” Skipping\")\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "global_unique_labels = np.unique(all_labels)\n",
        "global_label_map = {old_label: new_label for new_label, old_label in enumerate(global_unique_labels)}\n",
        "num_classes = len(global_unique_labels)\n",
        "print(f\"Global unique labels ({num_classes} classes): {global_unique_labels}\")\n",
        "\n",
        "# Step 2: Load client data, apply global label map and scale features\n",
        "client_datasets = []\n",
        "for client_id in range(1, NUM_CLIENTS + 1):\n",
        "    features_path = os.path.join(CLIENT_DATA_PATH, f\"client_{client_id}_features.csv\")\n",
        "    labels_path = os.path.join(CLIENT_DATA_PATH, f\"client_{client_id}_labels.csv\")\n",
        "\n",
        "    if os.path.exists(features_path) and os.path.exists(labels_path):\n",
        "        X = pd.read_csv(features_path, header=None).values  # shape: (N_samples, N_features)\n",
        "        y = pd.read_csv(labels_path, header=None).values.flatten()\n",
        "\n",
        "        if X.shape[0] != len(y):\n",
        "            print(f\"âš ï¸ Size mismatch for client {client_id} â€” Skipping (X: {X.shape}, y: {len(y)})\")\n",
        "            continue\n",
        "\n",
        "        # Map labels using global mapping\n",
        "        y_mapped = np.array([global_label_map[label] for label in y])\n",
        "\n",
        "        # Scale features per client (or you could fit scaler globally across all data if preferred)\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        client_datasets.append((client_id, X_scaled, y_mapped))\n",
        "    else:\n",
        "        print(f\"âŒ Missing files for client {client_id} â€” Skipping\")\n",
        "\n",
        "if not client_datasets:\n",
        "    raise RuntimeError(\"âŒ No valid client data available. Check dataset paths and integrity.\")\n",
        "\n",
        "print(f\"\\nâœ… Loaded clients: {[cid for cid, _, _ in client_datasets]}\")\n",
        "\n",
        "input_size = client_datasets[0][1].shape[1]\n",
        "\n",
        "# Define model\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Local training function\n",
        "def local_train(model, X, y):\n",
        "    model.train()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LR)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    for _ in range(EPOCHS_LOCAL):\n",
        "        for xb, yb in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(xb)\n",
        "            loss = loss_fn(output, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model.state_dict()\n",
        "\n",
        "# Federated averaging\n",
        "def average_weights(weight_list):\n",
        "    avg_weights = {}\n",
        "    for key in weight_list[0].keys():\n",
        "        avg_weights[key] = sum(weights[key] for weights in weight_list) / len(weight_list)\n",
        "    return avg_weights\n",
        "\n",
        "# Initialize global model\n",
        "global_model = MLPClassifier(input_size, num_classes).to(DEVICE)\n",
        "global_weights = global_model.state_dict()\n",
        "\n",
        "# Training rounds\n",
        "for rnd in range(ROUNDS):\n",
        "    print(f\"\\nğŸ“¡ --- Round {rnd + 1} ---\")\n",
        "    selected = random.sample(client_datasets, min(CLIENTS_PER_ROUND, len(client_datasets)))\n",
        "    local_weights = []\n",
        "\n",
        "    for client_id, X, y in selected:\n",
        "        print(f\" â†’ Training on client {client_id} with {len(y)} samples\")\n",
        "        client_model = MLPClassifier(input_size, num_classes).to(DEVICE)\n",
        "        client_model.load_state_dict(global_weights)\n",
        "        updated_weights = local_train(client_model, X, y)\n",
        "        local_weights.append(updated_weights)\n",
        "\n",
        "    global_weights = average_weights(local_weights)\n",
        "    global_model.load_state_dict(global_weights)\n",
        "\n",
        "print(\"\\nâœ… Federated training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "119zc5MDUDeA",
        "outputId": "6940c237-736d-4ba2-8b77-ecd3c1e60b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global unique labels (12 classes): [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
            "\n",
            "âœ… Loaded clients: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "\n",
            "ğŸ“¡ --- Round 1 ---\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            "\n",
            "ğŸ“¡ --- Round 2 ---\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            "\n",
            "ğŸ“¡ --- Round 3 ---\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            "\n",
            "ğŸ“¡ --- Round 4 ---\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            "\n",
            "ğŸ“¡ --- Round 5 ---\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            "\n",
            "âœ… Federated training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A federated learning experiment was conducted using 10 clients and a simple MLP model over 5 rounds, with 5 randomly selected clients participating in each round. A global label mapping and client-specific feature scaling were applied. All clients contributed at least once, with client 7 appearing most frequently and having the highest number of samples. The global model was updated through weight averaging, and training completed successfully without centralizing any data."
      ],
      "metadata": {
        "id": "22F82HO7Cmw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "\n",
        "# SETTINGS\n",
        "NUM_CLIENTS = 10\n",
        "CLIENT_DATA_PATH = \"/content/drive/Shareddrives/ML4Net/Seminar5/dataset_Seminar5/client_datasets/\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS_LOCAL = 20\n",
        "ROUNDS = 50\n",
        "CLIENTS_PER_ROUND = 5\n",
        "LR = 0.01\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Step 1: Collect all labels globally to build global label mapping\n",
        "all_labels = []\n",
        "\n",
        "for client_id in range(1, NUM_CLIENTS + 1):\n",
        "    labels_path = os.path.join(CLIENT_DATA_PATH, f\"client_{client_id}_labels.csv\")\n",
        "    if os.path.exists(labels_path):\n",
        "        y = pd.read_csv(labels_path, header=None).values.flatten()\n",
        "        all_labels.extend(y)\n",
        "    else:\n",
        "        print(f\"Missing labels file for client {client_id} â€” Skipping\")\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "global_unique_labels = np.unique(all_labels)\n",
        "global_label_map = {old_label: new_label for new_label, old_label in enumerate(global_unique_labels)}\n",
        "num_classes = len(global_unique_labels)\n",
        "print(f\"Global unique labels ({num_classes} classes): {global_unique_labels}\")\n",
        "\n",
        "# Step 2: Load client data, apply global label map and scale features\n",
        "client_datasets = []\n",
        "for client_id in range(1, NUM_CLIENTS + 1):\n",
        "    features_path = os.path.join(CLIENT_DATA_PATH, f\"client_{client_id}_features.csv\")\n",
        "    labels_path = os.path.join(CLIENT_DATA_PATH, f\"client_{client_id}_labels.csv\")\n",
        "\n",
        "    if os.path.exists(features_path) and os.path.exists(labels_path):\n",
        "        X = pd.read_csv(features_path, header=None).values  # shape: (N_samples, N_features)\n",
        "        y = pd.read_csv(labels_path, header=None).values.flatten()\n",
        "\n",
        "        if X.shape[0] != len(y):\n",
        "            print(f\"âš ï¸ Size mismatch for client {client_id} â€” Skipping (X: {X.shape}, y: {len(y)})\")\n",
        "            continue\n",
        "\n",
        "        # Map labels using global mapping\n",
        "        y_mapped = np.array([global_label_map[label] for label in y])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        if client_id == 1:\n",
        "          scaler_global = scaler\n",
        "\n",
        "\n",
        "        client_datasets.append((client_id, X_scaled, y_mapped))\n",
        "    else:\n",
        "        print(f\"Missing files for client {client_id} â€” Skipping\")\n",
        "\n",
        "if not client_datasets:\n",
        "    raise RuntimeError(\"No valid client data available. Check dataset paths and integrity.\")\n",
        "\n",
        "print(f\"\\nâœ… Loaded clients: {[cid for cid, _, _ in client_datasets]}\")\n",
        "\n",
        "input_size = client_datasets[0][1].shape[1]\n",
        "\n",
        "# Define model\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Local training function\n",
        "def local_train(model, X, y):\n",
        "    model.train()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LR)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    for _ in range(EPOCHS_LOCAL):\n",
        "        for xb, yb in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(xb)\n",
        "            loss = loss_fn(output, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model.state_dict()\n",
        "\n",
        "# Federated averaging with data-size-based weights (FedAvg)\n",
        "def weighted_average_weights(weight_list, data_sizes):\n",
        "    avg_weights = {}\n",
        "    total_data = sum(data_sizes)\n",
        "    for key in weight_list[0].keys():\n",
        "        weighted_sum = sum(weights[key] * (n_samples / total_data)\n",
        "                           for weights, n_samples in zip(weight_list, data_sizes))\n",
        "        avg_weights[key] = weighted_sum\n",
        "    return avg_weights\n",
        "\n",
        "\n",
        "# Initialize global model\n",
        "global_model = MLPClassifier(input_size, num_classes).to(DEVICE)\n",
        "global_weights = global_model.state_dict()\n",
        "\n",
        "for rnd in range(ROUNDS):\n",
        "    print(f\"\\nğŸ“¡ --- Round {rnd + 1} ---\")\n",
        "    selected = random.sample(client_datasets, min(CLIENTS_PER_ROUND, len(client_datasets)))\n",
        "    local_weights = []\n",
        "    local_sizes = []\n",
        "\n",
        "    for client_id, X, y in selected:\n",
        "        print(f\" â†’ Training on client {client_id} with {len(y)} samples\")\n",
        "        client_model = MLPClassifier(input_size, num_classes).to(DEVICE)\n",
        "        client_model.load_state_dict(global_weights)\n",
        "        updated_weights = local_train(client_model, X, y)\n",
        "        local_weights.append(updated_weights)\n",
        "        local_sizes.append(len(y))\n",
        "\n",
        "    # FedAvg ponderado\n",
        "    global_weights = weighted_average_weights(local_weights, local_sizes)\n",
        "    global_model.load_state_dict(global_weights)\n",
        "\n",
        "\n",
        "print(\"\\nâœ… Federated training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznzgaAOCqVR",
        "outputId": "8570f75b-d1f2-43bf-9386-e345a984554b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global unique labels (12 classes): [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
            "\n",
            "âœ… Loaded clients: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "\n",
            "ğŸ“¡ --- Round 1 ---\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            "\n",
            "ğŸ“¡ --- Round 2 ---\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            "\n",
            "ğŸ“¡ --- Round 3 ---\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            "\n",
            "ğŸ“¡ --- Round 4 ---\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            "\n",
            "ğŸ“¡ --- Round 5 ---\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 6 ---\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            "\n",
            "ğŸ“¡ --- Round 7 ---\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            "\n",
            "ğŸ“¡ --- Round 8 ---\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 9 ---\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            "\n",
            "ğŸ“¡ --- Round 10 ---\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 11 ---\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            "\n",
            "ğŸ“¡ --- Round 12 ---\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 13 ---\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            "\n",
            "ğŸ“¡ --- Round 14 ---\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            "\n",
            "ğŸ“¡ --- Round 15 ---\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 16 ---\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            "\n",
            "ğŸ“¡ --- Round 17 ---\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            "\n",
            "ğŸ“¡ --- Round 18 ---\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            "\n",
            "ğŸ“¡ --- Round 19 ---\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            "\n",
            "ğŸ“¡ --- Round 20 ---\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            "\n",
            "ğŸ“¡ --- Round 21 ---\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            "\n",
            "ğŸ“¡ --- Round 22 ---\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 23 ---\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            "\n",
            "ğŸ“¡ --- Round 24 ---\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            "\n",
            "ğŸ“¡ --- Round 25 ---\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            "\n",
            "ğŸ“¡ --- Round 26 ---\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            "\n",
            "ğŸ“¡ --- Round 27 ---\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            "\n",
            "ğŸ“¡ --- Round 28 ---\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 29 ---\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            "\n",
            "ğŸ“¡ --- Round 30 ---\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            "\n",
            "ğŸ“¡ --- Round 31 ---\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 32 ---\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            "\n",
            "ğŸ“¡ --- Round 33 ---\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 34 ---\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 35 ---\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            "\n",
            "ğŸ“¡ --- Round 36 ---\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            "\n",
            "ğŸ“¡ --- Round 37 ---\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            "\n",
            "ğŸ“¡ --- Round 38 ---\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            "\n",
            "ğŸ“¡ --- Round 39 ---\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            "\n",
            "ğŸ“¡ --- Round 40 ---\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            "\n",
            "ğŸ“¡ --- Round 41 ---\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            "\n",
            "ğŸ“¡ --- Round 42 ---\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            "\n",
            "ğŸ“¡ --- Round 43 ---\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            "\n",
            "ğŸ“¡ --- Round 44 ---\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            "\n",
            "ğŸ“¡ --- Round 45 ---\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            "\n",
            "ğŸ“¡ --- Round 46 ---\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            "\n",
            "ğŸ“¡ --- Round 47 ---\n",
            " â†’ Training on client 9 with 412 samples\n",
            " â†’ Training on client 4 with 207 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            "\n",
            "ğŸ“¡ --- Round 48 ---\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 7 with 448 samples\n",
            " â†’ Training on client 8 with 142 samples\n",
            "\n",
            "ğŸ“¡ --- Round 49 ---\n",
            " â†’ Training on client 8 with 142 samples\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 5 with 209 samples\n",
            "\n",
            "ğŸ“¡ --- Round 50 ---\n",
            " â†’ Training on client 3 with 365 samples\n",
            " â†’ Training on client 6 with 202 samples\n",
            " â†’ Training on client 1 with 314 samples\n",
            " â†’ Training on client 2 with 113 samples\n",
            " â†’ Training on client 10 with 64 samples\n",
            "\n",
            "âœ… Federated training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We improved the Federated Learning setup by implementing weighted model aggregation using the FedAvg algorithm. Instead of averaging client models equally, each client's contribution was weighted based on the size of its local dataset. This ensured a more accurate and fair update of the global model. The approach was integrated into the training loop and executed successfully across multiple rounds."
      ],
      "metadata": {
        "id": "AxqpdDjTDoJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare test data\n",
        "test_features_path = \"/content/drive/Shareddrives/ML4Net/Seminar5/dataset_Seminar5/test_features.csv\"\n",
        "test_labels_path = \"/content/drive/Shareddrives/ML4Net/Seminar5/dataset_Seminar5/test_labels.csv\"\n",
        "\n",
        "X_test = pd.read_csv(test_features_path, header=None).values\n",
        "y_test_raw = pd.read_csv(test_labels_path, header=None).values.flatten()\n",
        "\n",
        "# Apply the same label mapping as in training\n",
        "y_test = np.array([global_label_map[label] for label in y_test_raw])\n",
        "\n",
        "X_test_scaled = scaler_global.transform(X_test)\n",
        "\n",
        "\n",
        "# Convert to tensors\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(DEVICE)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "# Evaluation\n",
        "global_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = global_model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    total = y_test_tensor.size(0)\n",
        "    accuracy = correct / total\n",
        "\n",
        "print(f\"\\nğŸ“Š Test Accuracy of the global model: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScEnLawLDqSV",
        "outputId": "68e46992-ed82-4a63-dae9-5417b5494ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Test Accuracy of the global model: 51.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the final stage, we trained and evaluated the global model using Federated Learning (FL) across 10 clients with non-centralized Wi-Fi CSI data. We improved the model architecture with a deeper MLP and increased the training rounds to 50 and local epochs to 20. This setup significantly boosted performance, reaching a test accuracy of 50.40% on a 12-class classification task. The weighted averaging strategy (FedAvg) ensured fair contribution from clients based on their dataset sizes. Despite data heterogeneity, the model was able to learn meaningful patterns collaboratively. The results demonstrate the effectiveness of FL in preserving privacy while maintaining competitive performance."
      ],
      "metadata": {
        "id": "4fgYZyojIwfW"
      }
    }
  ]
}